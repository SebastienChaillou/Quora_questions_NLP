{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time as ti\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.stem import SnowballStemmer\n",
    "import gensim\n",
    "import importlib\n",
    "imported_module = importlib.import_module(\"Quora_classes\")\n",
    "importlib.reload(imported_module)\n",
    "#enables to update the .py file without restarting the notebook\n",
    "from Quora_classes import *\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The importance of cleaning the text\n",
    "\n",
    "https://www.kaggle.com/code/currie32/the-importance-of-cleaning-text/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "\n",
    "def text_to_wordlist(text, remove_stop_words = False, stem_words = False):\n",
    "    # Clean the text, with the option to remove stop_words and to stem words.\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    # text = re.sub(r\"what's\", \"\", text)\n",
    "    # text = re.sub(r\"What's\", \"\", text)\n",
    "    # text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r'\\s+', \" \", text) # remove new lines\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n",
    "    text = re.sub(r\"china\", \"China\", text)\n",
    "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\"quora\", \"Quora\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    # text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"America\", text)\n",
    "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
    "    text = re.sub(r\"Method\", \"method\", text)\n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/talha1503/quora-question-pairs-bi-lstm-pytorch\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",'i\\'m':'i am', \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled'}\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    text = text.lower()\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else mapping[t.lower()] if t.lower() in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "df['question1'] = df['question1'].apply(lambda x: clean_contractions(str(x),contraction_mapping))\n",
    "df['question2'] = df['question2'].apply(lambda x: clean_contractions(str(x),contraction_mapping))\n",
    "questions1 = df[\"question1\"].values\n",
    "questions2 = df[\"question2\"].values\n",
    "questions1 = [text_to_wordlist(question) for question in questions1]\n",
    "questions2 = [text_to_wordlist(question) for question in questions2]\n",
    "df[\"question1\"] = questions1\n",
    "df[\"question2\"] = questions2\n",
    "columns = [\"question1\", \"question2\", \"is_duplicate\"] #only keeps relevant columns\n",
    "df = df[columns]\n",
    "df.to_csv(\"clean_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We train a Word2Vec model\n",
    "\n",
    "As we have many sentences\n",
    "\n",
    "https://www.kaggle.com/code/liananapalkova/simply-about-word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_questions = np.concatenate([df[\"question1\"].values, df[\"question2\"].values])\n",
    "temps1 = ti.perf_counter()\n",
    "All_questions = [gensim.utils.simple_preprocess(question.encode('utf-8')) for question in All_questions]\n",
    "# converts questions to a list of words\n",
    "print(ti.perf_counter() - temps1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_size = 200 #size of the embedding\n",
    "model_W2V = gensim.models.Word2Vec(vector_size=v_size, window=10, min_count=2, sg=1, workers=10)\n",
    "model_W2V.build_vocab(All_questions)  # prepare the model vocabulary\n",
    "\n",
    "temps1 = ti.perf_counter()\n",
    "model_W2V.train(All_questions, total_examples = len(All_questions), epochs = 10)\n",
    "print(ti.perf_counter() - temps1)\n",
    "model_W2V.save(\"quoraW2V.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.630799407351708\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "df = pd.read_csv(\"clean_train.csv\", index_col=[0])\n",
    "print(1 - df[\"is_duplicate\"].mean())\n",
    "questions1 = df[\"question1\"]\n",
    "questions2 = df[\"question2\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202143, 3) (202144, 3) (10108, 3)\n"
     ]
    }
   ],
   "source": [
    "v_size = 200\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "model_W2V = gensim.models.Word2Vec.load(\"quoraW2V.model\")\n",
    "\n",
    "df_train, df_test0 = train_test_split(df, test_size = 0.5, random_state = 0)\n",
    "df_test, df_valid = train_test_split(df_test0, test_size = 0.05, random_state = 0)\n",
    "print(df_train.shape, df_test0.shape, df_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We train a Word2Vec model\n",
    "\n",
    "As we have many sentences\n",
    "\n",
    "https://www.kaggle.com/code/liananapalkova/simply-about-word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = model_W2V.wv.index_to_key #list of vectorized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.45613769e-01,  1.92996264e-01, -1.00261658e-01,\n",
       "        -3.68416645e-02, -2.71115869e-01, -1.28229961e-01,\n",
       "         3.70359838e-01,  6.59089267e-01, -2.92757541e-01,\n",
       "         1.45318314e-01,  1.21351726e-01, -2.28266105e-01,\n",
       "         1.31480336e-01,  9.00155008e-02,  1.97913811e-01,\n",
       "         1.69324070e-01,  5.96522167e-03,  2.28195831e-01,\n",
       "         4.29359972e-02, -7.59958997e-02, -3.59218158e-02,\n",
       "         1.68801025e-01,  9.68624502e-02, -5.76503917e-05,\n",
       "         3.32433544e-03,  1.68660656e-01,  1.08743578e-01,\n",
       "        -7.24220052e-02,  5.98046221e-02,  1.52336255e-01,\n",
       "         8.43257159e-02,  6.14712358e-01,  8.77336115e-02,\n",
       "        -6.01550996e-01,  1.26501873e-01,  4.43453133e-01,\n",
       "        -7.81522244e-02, -1.03310399e-01,  3.12871896e-02,\n",
       "        -1.19687356e-01,  2.26614505e-01,  1.48417041e-01,\n",
       "         3.96981835e-02, -5.58917271e-03, -3.00633669e-01,\n",
       "        -3.54911715e-01,  1.77101851e-01, -1.55253515e-01,\n",
       "         3.46112341e-01,  1.16723992e-01, -2.17020094e-01,\n",
       "        -3.22902352e-01, -4.18753147e-01, -5.46190977e-01,\n",
       "         1.10304579e-01, -2.12674230e-01, -3.24964643e-01,\n",
       "        -3.03451091e-01,  1.72081411e-01,  1.84784532e-01,\n",
       "         1.73662543e-01,  1.62665904e-01,  6.07082903e-01,\n",
       "        -4.87397403e-01,  2.00556234e-01,  4.35854048e-02,\n",
       "         3.40093113e-02,  1.95563436e-01, -1.17698267e-01,\n",
       "         3.64415944e-01,  2.59140819e-01, -4.93889488e-02,\n",
       "         6.42613113e-01, -9.67081413e-02,  4.97546941e-02,\n",
       "        -9.30125639e-02,  4.98097450e-01, -1.56778619e-01,\n",
       "         7.20925108e-02, -7.54091203e-01,  2.79828697e-01,\n",
       "        -2.17829272e-01, -2.56651074e-01,  4.13558990e-01,\n",
       "        -8.89840499e-02,  2.86394447e-01,  3.18336546e-01,\n",
       "         2.95466661e-01, -2.00083956e-01, -3.22225004e-01,\n",
       "         2.32142478e-01,  1.63012162e-01, -1.74218304e-02,\n",
       "         3.14701319e-01, -1.22523218e-01,  3.35050583e-01,\n",
       "         4.03284878e-02,  4.67578182e-03,  2.78633654e-01,\n",
       "         2.50634621e-03, -1.62999049e-01,  3.42987746e-01,\n",
       "         3.29342745e-02, -1.67090908e-01, -7.90295750e-02,\n",
       "        -1.31803989e-01,  3.67234945e-01,  9.53790471e-02,\n",
       "        -2.97963291e-01, -1.41469976e-02,  2.66944859e-02,\n",
       "        -1.79172397e-01,  9.61942747e-02,  1.48501188e-01,\n",
       "         1.29122466e-01,  7.23371804e-02,  1.48460269e-01,\n",
       "        -5.16176105e-01, -3.21429044e-01, -6.23929620e-01,\n",
       "         1.26592293e-02,  2.75056660e-01,  9.36035160e-03,\n",
       "        -1.37087982e-02, -1.46753579e-01,  1.83957577e-01,\n",
       "        -1.32886797e-01, -3.74846607e-01, -1.43717051e-01,\n",
       "        -7.55657479e-02,  3.90794307e-01,  1.15354039e-01,\n",
       "         8.87660086e-02, -3.58575359e-02,  1.08864635e-01,\n",
       "         2.95051873e-01, -1.17620833e-01,  1.81266308e-01,\n",
       "        -2.95564592e-01, -9.73469168e-02,  3.52922767e-01,\n",
       "        -3.60757206e-03, -4.25377302e-02,  3.12478095e-01,\n",
       "         2.50508368e-01,  9.04304162e-02,  4.91859242e-02,\n",
       "         1.94009721e-01, -1.88404709e-01,  1.27236713e-02,\n",
       "        -1.95938557e-01,  3.10994595e-01,  3.25176120e-01,\n",
       "        -9.90852341e-02, -4.68336135e-01,  2.09619850e-01,\n",
       "         8.33090395e-02,  1.16441250e-01, -1.34432569e-01,\n",
       "         3.66883092e-02,  2.89255232e-01, -2.63866726e-02,\n",
       "        -2.71469168e-03,  8.55551288e-02, -1.03219777e-01,\n",
       "         5.32799363e-01,  7.35772252e-02,  3.45292650e-02,\n",
       "        -1.49351418e-01,  5.79243526e-02, -3.16317230e-01,\n",
       "        -7.01319128e-02,  1.57658324e-01,  2.63170630e-01,\n",
       "         4.42375660e-01,  1.25669409e-02, -4.53160048e-01,\n",
       "         2.19543532e-01,  2.24501997e-01,  1.12344854e-01,\n",
       "        -1.25380665e-01,  1.23666614e-01, -3.33658397e-01,\n",
       "        -3.49970013e-01,  2.80331731e-01,  4.43551004e-01,\n",
       "        -2.96006709e-01, -1.61790669e-01,  4.68168050e-01,\n",
       "         2.77367443e-01,  1.33748502e-01,  4.03389543e-01,\n",
       "         1.44180566e-01, -1.80174991e-01,  1.00621045e-01,\n",
       "         3.38443100e-01, -2.41546616e-01,  7.37305582e-02,\n",
       "        -3.97757202e-01,  7.30929598e-02],\n",
       "       [ 1.34860754e-01,  9.73449722e-02,  1.11467317e-01,\n",
       "        -1.29153237e-01, -5.75056262e-02, -1.33420914e-01,\n",
       "         4.34291065e-02,  1.99046955e-01, -5.32092154e-02,\n",
       "        -4.81313206e-02,  1.55043840e-01,  2.70760730e-02,\n",
       "         6.48866966e-02, -1.67254090e-01,  1.09893167e-02,\n",
       "        -1.93382218e-01,  1.20974362e-01,  4.04129714e-01,\n",
       "        -8.97862092e-02, -2.02780202e-01,  2.84940571e-01,\n",
       "         1.83345214e-01,  2.92723566e-01, -7.78519586e-02,\n",
       "         8.68007690e-02,  9.20852926e-03,  1.15379743e-01,\n",
       "         4.35586683e-02, -2.67096549e-01, -9.09105409e-03,\n",
       "        -2.64906824e-01, -1.53384745e-01,  8.50311294e-02,\n",
       "        -9.06818882e-02,  2.80418247e-02, -1.20596275e-01,\n",
       "         2.42267475e-01,  1.23113059e-02,  1.05762325e-01,\n",
       "         6.64244080e-03,  1.13196634e-01, -1.05673157e-01,\n",
       "        -9.70375389e-02, -3.42859514e-02,  3.36304195e-02,\n",
       "        -3.63894366e-02, -1.56281874e-01, -9.11693554e-03,\n",
       "         2.00119630e-01,  2.78463382e-02,  2.91209444e-02,\n",
       "         1.18215688e-01, -2.38558967e-02, -6.15367386e-03,\n",
       "        -6.84087202e-02,  5.09094298e-02, -3.56997177e-02,\n",
       "        -8.89896899e-02,  9.40241944e-03,  2.44952545e-01,\n",
       "        -2.09591165e-01, -1.97298586e-01,  1.81263164e-01,\n",
       "         8.89579430e-02, -4.61160531e-03,  1.96657583e-01,\n",
       "         7.36963528e-04,  3.51167560e-01, -4.35207449e-02,\n",
       "         1.72023118e-01,  2.76938900e-02,  5.13629839e-02,\n",
       "         4.41030413e-01,  5.45355491e-02,  1.41996801e-01,\n",
       "         2.25857973e-01,  2.09948272e-01, -5.52899018e-02,\n",
       "         2.35121381e-02,  1.46988332e-01, -1.75405424e-02,\n",
       "        -1.07975878e-01, -1.40523881e-01,  2.21715704e-01,\n",
       "         7.92937055e-02, -2.56073177e-01,  1.05963379e-01,\n",
       "        -1.10890128e-01, -9.61255059e-02, -2.67794486e-02,\n",
       "         4.03248556e-02,  1.50806785e-01,  1.98867291e-01,\n",
       "         2.09062755e-01,  1.41339883e-01,  6.68812096e-02,\n",
       "         5.45491092e-02,  2.32194945e-01,  1.07768238e-01,\n",
       "         8.84414613e-02,  1.62128180e-01,  1.79177374e-02,\n",
       "         2.04561815e-01, -7.35082403e-02, -1.48652317e-02,\n",
       "        -1.54311806e-01,  1.10472649e-01,  1.67423069e-01,\n",
       "        -1.85971975e-01, -1.97090611e-01,  3.87614518e-02,\n",
       "        -1.90496370e-01,  1.47365019e-01, -1.55377150e-01,\n",
       "        -1.43087120e-03, -4.92398031e-02, -1.22816503e-01,\n",
       "        -1.04990669e-01,  2.63973065e-02, -5.81547432e-02,\n",
       "         7.16638118e-02,  2.07405835e-01,  4.35255796e-01,\n",
       "        -9.43872482e-02, -7.80672655e-02, -1.05296187e-02,\n",
       "        -9.06469151e-02, -4.62708175e-02, -7.75262713e-02,\n",
       "         9.04301181e-02,  8.50306675e-02, -1.55986294e-01,\n",
       "        -1.66376352e-01, -1.28600940e-01,  2.36875802e-01,\n",
       "         5.23269713e-01,  2.43758097e-01, -1.08007759e-01,\n",
       "        -1.66651651e-01,  6.69811592e-02,  1.00992911e-01,\n",
       "        -1.35604978e-01, -2.21886307e-01, -1.19267672e-01,\n",
       "        -3.19610303e-03,  9.82089937e-02,  4.42637280e-02,\n",
       "         1.14342012e-01,  9.71344337e-02,  9.03689787e-02,\n",
       "         2.14589313e-01, -8.57985094e-02,  2.30521515e-01,\n",
       "        -4.05438393e-02, -3.94654483e-01,  1.21732697e-01,\n",
       "         1.70048624e-01,  9.32664201e-02, -1.74663633e-01,\n",
       "        -4.96534444e-02,  2.90702015e-01,  3.12116712e-01,\n",
       "        -9.29081813e-02, -9.34075788e-02, -2.03117773e-01,\n",
       "         5.42151868e-01,  2.36905250e-03, -8.66037235e-02,\n",
       "         3.51916347e-03,  9.80500877e-02, -4.94424440e-02,\n",
       "        -1.62227135e-02,  4.64221276e-02,  4.55483757e-02,\n",
       "        -9.09123495e-02, -3.08263022e-03, -2.89918244e-01,\n",
       "         2.87819207e-02,  1.89302370e-01,  2.08761975e-01,\n",
       "        -4.08997625e-01,  2.22061619e-01, -7.54298468e-04,\n",
       "         1.04187071e-01,  1.90211862e-01,  1.15297407e-01,\n",
       "        -2.64435429e-02, -2.37591758e-01,  2.19140977e-01,\n",
       "         2.17808560e-01,  1.30070075e-01, -3.70997079e-02,\n",
       "        -3.40436399e-01, -2.13323593e-01, -1.01300120e-01,\n",
       "        -1.67313684e-02,  1.68704525e-01,  2.27724746e-01,\n",
       "         1.01676948e-01, -1.46599010e-01]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_W2V.wv[[\"america\", \"the\"]]  #can convert a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    \"\"\"Custom method to treat the mini-batch before loading\"\"\"\n",
    "    text_batch, labels = deepcopy(zip(*data)) #need to copy otherwise changes in dataset\n",
    "    bs = len(text_batch)\n",
    "    tensor1 = torch.zeros(bs, MAX_SEQUENCE_LENGTH, v_size)\n",
    "    tensor2 = torch.zeros(bs, MAX_SEQUENCE_LENGTH, v_size)\n",
    "    for n, text_dico in enumerate(text_batch):\n",
    "        question1 = text_dico[\"question1\"]\n",
    "        question2 = text_dico[\"question2\"]\n",
    "        try:\n",
    "            embedding_array1 = model_W2V.wv[question1]\n",
    "            embedding_array2 = model_W2V.wv[question2]\n",
    "        except ValueError:\n",
    "            continue #keeps 0\n",
    "        tensor1[n, :len(question1)] = torch.from_numpy(embedding_array1)\n",
    "        tensor2[n, :len(question2)] = torch.from_numpy(embedding_array2)\n",
    "    dico_return = {\"question1\" : tensor1, \"question2\" : tensor2}\n",
    "    labels = torch.tensor(labels).float().reshape(1, -1)\n",
    "    return dico_return, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Quora_dataset2(df_train, vocab)\n",
    "valid_set = Quora_dataset2(df_valid, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "params = {'batch_size' : bs,\n",
    "          'shuffle' : True,\n",
    "          'num_workers' : 0,\n",
    "          'collate_fn' : custom_collate}\n",
    "trainloader = torch.utils.data.DataLoader(train_set, **params)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm1 = nn.LSTM(v_size, hidden_size, batch_first = True, bidirectional = True)\n",
    "        self.lstm2 = nn.LSTM(v_size, hidden_size, batch_first = True, bidirectional = True)\n",
    "        self.fc1 = nn.Linear(hidden_size * 4 * MAX_SEQUENCE_LENGTH, hidden_size * 2)\n",
    "        #self.batch_norm = x = nn.BatchNorm1d(hidden_size * 2) #not very useful\n",
    "        self.fc2 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        self.drop = nn.Dropout(.2)\n",
    "\n",
    "    def __call__(self, x_dic):\n",
    "        x1 = x_dic[\"question1\"]\n",
    "        x2 = x_dic[\"question2\"]\n",
    "        x1 = x1.cuda()\n",
    "        x2 = x2.cuda()\n",
    "        x11 = self.lstm1(x1)\n",
    "        x22 = self.lstm2(x2)\n",
    "        x = torch.cat([x11[0], x22[0]], axis = 2) #concatenate for FC layer\n",
    "        x = x.view(-1, MAX_SEQUENCE_LENGTH * self.hidden_size * 4) #flattens\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(self.drop(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = SimpleModel(50)\n",
    "model_simple.to(device)\n",
    "n_epochs = 1\n",
    "lr = .01\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_simple.parameters(),lr = lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1,   500] loss: 0.0187\n",
      " [1,  1000] loss: 0.0178\n",
      " [1,  1500] loss: 0.0174\n",
      " [1,  2000] loss: 0.0175\n",
      " [1,  2500] loss: 0.0171\n",
      " [1,  3000] loss: 0.0172\n",
      " [1,  3500] loss: 0.0172\n",
      " [1,  4000] loss: 0.0170\n",
      " [1,  4500] loss: 0.0169\n",
      " [1,  5000] loss: 0.0169\n",
      " [1,  5500] loss: 0.0171\n",
      " [1,  6000] loss: 0.0168\n",
      " validation loss: 0.0168\n",
      " validation accuracy: 0.7210\n",
      "     106.88213540000015\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "temps1 = ti.perf_counter()\n",
    "total = len(train_set)\n",
    "dis_loss = 500\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0\n",
    "    model_simple.train()\n",
    "    for i, data in enumerate(trainloader):\n",
    "        x_dic, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_simple(x_dic).T\n",
    "        loss = criterion(outputs, labels.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 0:\n",
    "            print('\\r', '{:.2%}'.format(i / (total // bs)), end = '', sep = '')\n",
    "        if i % dis_loss == dis_loss - 1 or i == 0:    # print every dis_loss mini-batches\n",
    "            print('\\r', f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / dis_loss / bs:.4f}')\n",
    "            running_loss = 0.0\n",
    "        # if i == 0:\n",
    "        #     break\n",
    "\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "    model_simple.eval()\n",
    "    for i, data in enumerate(valid_loader):\n",
    "        x_dic, labels = data\n",
    "        outputs = model_simple(x_dic).T\n",
    "        loss = criterion(outputs, labels.cuda())\n",
    "        running_loss += loss.item()\n",
    "        sigmoid = nn.Sigmoid()(outputs)\n",
    "        predictions = torch.round(sigmoid)\n",
    "        running_accuracy += (predictions == labels.cuda()).sum()\n",
    "    print('\\r', f\"validation loss: {running_loss / df_valid.shape[0]:.4f}\")\n",
    "    print('\\r', f\"validation accuracy: {running_accuracy / df_valid.shape[0]:.4f}\")\n",
    "\n",
    "print(\"    \", ti.perf_counter() - temps1)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
