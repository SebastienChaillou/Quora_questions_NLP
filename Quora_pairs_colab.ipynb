{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIsAR3WjVzXt",
        "outputId": "a6cdf446-c531-404b-ef1b-20858150d17e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.2.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import *\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time as ti\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from string import punctuation\n",
        "from nltk.stem import SnowballStemmer\n",
        "!pip install gensim --upgrade \n",
        "import gensim\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RNjdh6GgVzXu"
      },
      "outputs": [],
      "source": [
        "class Quora_dataset2(torch.utils.data.Dataset):\n",
        "    # enable to create a dataset for the data loader\n",
        "    # https://towardsdatascience.com/building-efficient-custom-datasets-in-pytorch-2563b946fd9f\n",
        "    def __init__(self, df1, vocab):\n",
        "        self.df1 = df1\n",
        "        self.MAX_SEQUENCE_LENGTH = 30\n",
        "        self.vocab = vocab\n",
        "        self.prepare_loader()\n",
        "\n",
        "    def prepare_loader(self):\n",
        "        # create to dictionnary for each row of the dataframe\n",
        "        values = self.df1.values\n",
        "        cles = self.df1.columns\n",
        "        questions = []\n",
        "        for vals in values:\n",
        "            #converts into a list:\n",
        "            question1_list = gensim.utils.simple_preprocess(vals[0].encode('utf-8'))\n",
        "            question1_list = [word for word in question1_list if word in self.vocab][:self.MAX_SEQUENCE_LENGTH]\n",
        "            question2_list = gensim.utils.simple_preprocess(vals[0].encode('utf-8'))\n",
        "            question2_list = [word for word in question2_list if word in self.vocab][:self.MAX_SEQUENCE_LENGTH]\n",
        "            questions.append({cles[0] : question1_list, cles[1] : question2_list})\n",
        "        self.sample = list(zip(questions, values[:, 2]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sample[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBLdighmV18b",
        "outputId": "51976ee4-d5ec-42b5-8f2c-37a22db08221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "05Fcp_PMexJe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/')\n",
        "df = pd.read_csv(\"train.csv.zip\")\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2zl8-34VzXw"
      },
      "source": [
        "## The importance of cleaning the text\n",
        "\n",
        "https://www.kaggle.com/code/currie32/the-importance-of-cleaning-text/notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HpnXHs45VzXx"
      },
      "outputs": [],
      "source": [
        "stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
        "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
        "              'Is','If','While','This']\n",
        "\n",
        "def text_to_wordlist(text, remove_stop_words = False, stem_words = False):\n",
        "    # Clean the text, with the option to remove stop_words and to stem words.\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
        "    # text = re.sub(r\"what's\", \"\", text)\n",
        "    # text = re.sub(r\"What's\", \"\", text)\n",
        "    # text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r'\\s+', \" \", text) # remove new lines\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"I'm\", \"I am\", text)\n",
        "    text = re.sub(r\" m \", \" am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e-mail\", \"email\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
        "    text = re.sub(r\" usa \", \" America \", text)\n",
        "    text = re.sub(r\" USA \", \" America \", text)\n",
        "    text = re.sub(r\" u s \", \" America \", text)\n",
        "    text = re.sub(r\" uk \", \" England \", text)\n",
        "    text = re.sub(r\" UK \", \" England \", text)\n",
        "    text = re.sub(r\"india\", \"India\", text)\n",
        "    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n",
        "    text = re.sub(r\"china\", \"China\", text)\n",
        "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
        "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
        "    text = re.sub(r\"intially\", \"initially\", text)\n",
        "    text = re.sub(r\"quora\", \"Quora\", text)\n",
        "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
        "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
        "    text = re.sub(r\"actived\", \"active\", text)\n",
        "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
        "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
        "    text = re.sub(r\" cs \", \" computer science \", text) \n",
        "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
        "    # text = re.sub(r\" iPhone \", \" phone \", text)\n",
        "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
        "    text = re.sub(r\"calender\", \"calendar\", text)\n",
        "    text = re.sub(r\"ios\", \"operating system\", text)\n",
        "    text = re.sub(r\"gps\", \"GPS\", text)\n",
        "    text = re.sub(r\"gst\", \"GST\", text)\n",
        "    text = re.sub(r\"programing\", \"programming\", text)\n",
        "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
        "    text = re.sub(r\"dna\", \"DNA\", text)\n",
        "    text = re.sub(r\"III\", \"3\", text) \n",
        "    text = re.sub(r\"the US\", \"America\", text)\n",
        "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
        "    text = re.sub(r\"Method\", \"method\", text)\n",
        "    text = re.sub(r\"Find\", \"find\", text) \n",
        "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
        "    text = re.sub(r\" J K \", \" JK \", text)\n",
        "    \n",
        "    # Remove punctuation from text\n",
        "    text = ''.join([c for c in text if c not in punctuation])\n",
        "    \n",
        "    # Optionally, remove stop words\n",
        "    if remove_stop_words:\n",
        "        text = text.split()\n",
        "        text = [w for w in text if not w in stop_words]\n",
        "        text = \" \".join(text)\n",
        "    \n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "    \n",
        "    # Return a list of words\n",
        "    return(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iFe-cqW9VzXy"
      },
      "outputs": [],
      "source": [
        "#https://www.kaggle.com/code/talha1503/quora-question-pairs-bi-lstm-pytorch\n",
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",'i\\'m':'i am', \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled'}\n",
        "\n",
        "def clean_contractions(text, mapping):\n",
        "    text = text.lower()\n",
        "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
        "    for s in specials:\n",
        "        text = text.replace(s, \"'\")\n",
        "    text = ' '.join([mapping[t] if t in mapping else mapping[t.lower()] if t.lower() in mapping else t for t in text.split(\" \")])\n",
        "    return text\n",
        "\n",
        "df['question1'] = df['question1'].apply(lambda x: clean_contractions(str(x),contraction_mapping))\n",
        "df['question2'] = df['question2'].apply(lambda x: clean_contractions(str(x),contraction_mapping))\n",
        "questions1 = df[\"question1\"].values\n",
        "questions2 = df[\"question2\"].values\n",
        "questions1 = [text_to_wordlist(question) for question in questions1]\n",
        "questions2 = [text_to_wordlist(question) for question in questions2]\n",
        "df[\"question1\"] = questions1\n",
        "df[\"question2\"] = questions2\n",
        "columns = [\"question1\", \"question2\", \"is_duplicate\"] #only keeps relevant columns\n",
        "df = df[columns]\n",
        "df.to_csv(\"clean_train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pki_MjNHVzXz"
      },
      "source": [
        "## We train a Word2Vec model\n",
        "\n",
        "As we have many sentences\n",
        "\n",
        "https://www.kaggle.com/code/liananapalkova/simply-about-word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2ckdZ1qVzX0",
        "outputId": "d7534ee8-8850-4816-d4a6-86bd308180c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27.305018055000005\n"
          ]
        }
      ],
      "source": [
        "All_questions = np.concatenate([df[\"question1\"].values, df[\"question2\"].values])\n",
        "temps1 = ti.perf_counter()\n",
        "All_questions = [gensim.utils.simple_preprocess(question.encode('utf-8')) for question in All_questions]\n",
        "# converts questions to a list of words\n",
        "print(ti.perf_counter() - temps1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXEOBgvHVzX0",
        "outputId": "40091630-d6a3-49c2-a72e-0d1a692ae4e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "441.22376084499996\n"
          ]
        }
      ],
      "source": [
        "v_size = 200 #size of the embedding\n",
        "model_W2V = gensim.models.Word2Vec(vector_size=v_size, window=10, min_count=2, sg=1, workers=10)\n",
        "model_W2V.build_vocab(All_questions)  # prepare the model vocabulary\n",
        "\n",
        "temps1 = ti.perf_counter()\n",
        "model_W2V.train(All_questions, total_examples = len(All_questions), epochs = 10)\n",
        "print(ti.perf_counter() - temps1)\n",
        "model_W2V.save(\"quoraW2V.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDm8p8u7VzX0",
        "outputId": "ef5f3c35-6ca4-45ae-db8f-49e3972e705b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.630799407351708\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "df = pd.read_csv(\"clean_train.csv\", index_col=[0])\n",
        "print(1 - df[\"is_duplicate\"].mean())\n",
        "questions1 = df[\"question1\"]\n",
        "questions2 = df[\"question2\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbqbVyarVzX1",
        "outputId": "c1ae3077-7d76-44ae-8abc-94113f17b1e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(363858, 3) (40429, 3) (2022, 3)\n"
          ]
        }
      ],
      "source": [
        "v_size = 200\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "model_W2V = gensim.models.Word2Vec.load(\"quoraW2V.model\")\n",
        "\n",
        "df_train, df_test0 = train_test_split(df, test_size = 0.1, random_state = 0)\n",
        "df_test, df_valid = train_test_split(df_test0, test_size = 0.05, random_state = 0)\n",
        "print(df_train.shape, df_test0.shape, df_valid.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0GCyuSHVzX2"
      },
      "source": [
        "## We train a Word2Vec model\n",
        "\n",
        "As we have many sentences\n",
        "\n",
        "https://www.kaggle.com/code/liananapalkova/simply-about-word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "u0a9tyFqVzX2"
      },
      "outputs": [],
      "source": [
        "vocab = model_W2V.wv.index_to_key #list of vectorized words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bMCbvb_aVzX2"
      },
      "outputs": [],
      "source": [
        "def custom_collate(data):\n",
        "    \"\"\"Custom method to treat the mini-batch before loading\"\"\"\n",
        "    text_batch, labels = deepcopy(zip(*data)) #need to copy otherwise changes in dataset\n",
        "    bs = len(text_batch)\n",
        "    tensor1 = torch.zeros(bs, MAX_SEQUENCE_LENGTH, v_size)\n",
        "    tensor2 = torch.zeros(bs, MAX_SEQUENCE_LENGTH, v_size)\n",
        "    for n, text_dico in enumerate(text_batch):\n",
        "        question1 = text_dico[\"question1\"]\n",
        "        question2 = text_dico[\"question2\"]\n",
        "        try:\n",
        "            embedding_array1 = model_W2V.wv[question1]\n",
        "            embedding_array2 = model_W2V.wv[question2]\n",
        "        except ValueError:\n",
        "            continue #keeps 0\n",
        "        tensor1[n, :len(question1)] = torch.from_numpy(embedding_array1)\n",
        "        tensor2[n, :len(question2)] = torch.from_numpy(embedding_array2)\n",
        "    dico_return = {\"question1\" : tensor1, \"question2\" : tensor2}\n",
        "    labels = torch.tensor(labels).float().reshape(1, -1)\n",
        "    return dico_return, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cC4PSRpUVzX3"
      },
      "outputs": [],
      "source": [
        "train_set = Quora_dataset2(df_train, vocab)\n",
        "valid_set = Quora_dataset2(df_valid, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "YzwfH8wmVzX3"
      },
      "outputs": [],
      "source": [
        "bs = 32\n",
        "params = {'batch_size' : bs,\n",
        "          'shuffle' : True,\n",
        "          'num_workers' : 0,\n",
        "          'collate_fn' : custom_collate}\n",
        "trainloader = torch.utils.data.DataLoader(train_set, **params)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, **params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "rbQqOnu2VzX3"
      },
      "outputs": [],
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm1 = nn.LSTM(v_size, hidden_size, batch_first = True, bidirectional = True)\n",
        "        self.lstm2 = nn.LSTM(v_size, hidden_size, batch_first = True, bidirectional = True)\n",
        "        self.fc1 = nn.Linear(hidden_size * 4 * MAX_SEQUENCE_LENGTH, hidden_size * 2)\n",
        "        self.fc2 = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, 1)\n",
        "        self.drop = nn.Dropout(.2)\n",
        "\n",
        "    def __call__(self, x_dic):\n",
        "        x1 = x_dic[\"question1\"]\n",
        "        x2 = x_dic[\"question2\"]\n",
        "        x1 = x1.cuda()\n",
        "        x2 = x2.cuda()\n",
        "        x11 = self.lstm1(x1)\n",
        "        x22 = self.lstm2(x2)\n",
        "        x = torch.cat([x11[0], x22[0]], axis = 2) #concatenate for FC layer\n",
        "        x = x.view(-1, MAX_SEQUENCE_LENGTH * self.hidden_size * 4) #flattens\n",
        "        #print(x.shape)\n",
        "        x = F.relu(self.fc1(self.drop(x)))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "EgOjAsoHermw"
      },
      "outputs": [],
      "source": [
        "model_simple = SimpleModel(50)\n",
        "model_simple.to(device)\n",
        "n_epochs = 30\n",
        "lr = .001\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model_simple.parameters(),lr = lr) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKIPnlJ7VzX4",
        "outputId": "c508922b-421e-4810-bc26-782a1f4f58de",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " [1,     1] loss: 0.0000\n",
            " [1,   500] loss: 0.0185\n",
            " [1,  1000] loss: 0.0175\n",
            " [1,  1500] loss: 0.0174\n",
            " [1,  2000] loss: 0.0172\n",
            " [1,  2500] loss: 0.0168\n",
            " [1,  3000] loss: 0.0169\n",
            " [1,  3500] loss: 0.0168\n",
            " [1,  4000] loss: 0.0168\n",
            " [1,  4500] loss: 0.0165\n",
            " [1,  5000] loss: 0.0166\n",
            " [1,  5500] loss: 0.0165\n",
            " [1,  6000] loss: 0.0165\n",
            " [1,  6500] loss: 0.0166\n",
            " [1,  7000] loss: 0.0165\n",
            " [1,  7500] loss: 0.0161\n",
            " [1,  8000] loss: 0.0164\n",
            " [1,  8500] loss: 0.0162\n",
            " [1,  9000] loss: 0.0161\n",
            " [1,  9500] loss: 0.0161\n",
            " [1, 10000] loss: 0.0161\n",
            " [1, 10500] loss: 0.0161\n",
            " [1, 11000] loss: 0.0161\n",
            " validation loss: 0.0161\n",
            " validation accuracy: 0.7537\n",
            " [2,     1] loss: 0.0000\n",
            " [2,   500] loss: 0.0156\n",
            " [2,  1000] loss: 0.0155\n",
            " [2,  1500] loss: 0.0153\n",
            " [2,  2000] loss: 0.0154\n",
            " [2,  2500] loss: 0.0154\n",
            " [2,  3000] loss: 0.0154\n",
            " [2,  3500] loss: 0.0157\n",
            " [2,  4000] loss: 0.0154\n",
            " [2,  4500] loss: 0.0152\n",
            " [2,  5000] loss: 0.0155\n",
            " [2,  5500] loss: 0.0155\n",
            " [2,  6000] loss: 0.0153\n",
            " [2,  6500] loss: 0.0154\n",
            " [2,  7000] loss: 0.0153\n",
            " [2,  7500] loss: 0.0154\n",
            " [2,  8000] loss: 0.0153\n",
            " [2,  8500] loss: 0.0153\n",
            " [2,  9000] loss: 0.0152\n",
            " [2,  9500] loss: 0.0153\n",
            " [2, 10000] loss: 0.0151\n",
            " [2, 10500] loss: 0.0153\n",
            " [2, 11000] loss: 0.0151\n",
            " validation loss: 0.0157\n",
            " validation accuracy: 0.7596\n",
            " [3,     1] loss: 0.0000\n",
            " [3,   500] loss: 0.0144\n",
            " [3,  1000] loss: 0.0142\n",
            " [3,  1500] loss: 0.0144\n",
            " [3,  2000] loss: 0.0142\n",
            " [3,  2500] loss: 0.0139\n",
            " [3,  3000] loss: 0.0142\n",
            " [3,  3500] loss: 0.0143\n",
            " [3,  4000] loss: 0.0143\n",
            " [3,  4500] loss: 0.0144\n",
            " [3,  5000] loss: 0.0144\n",
            " [3,  5500] loss: 0.0141\n",
            " [3,  6000] loss: 0.0143\n",
            " [3,  6500] loss: 0.0143\n",
            " [3,  7000] loss: 0.0145\n",
            " [3,  7500] loss: 0.0143\n",
            " [3,  8000] loss: 0.0140\n",
            " [3,  8500] loss: 0.0143\n",
            " [3,  9000] loss: 0.0143\n",
            " [3,  9500] loss: 0.0142\n",
            " [3, 10000] loss: 0.0141\n",
            " [3, 10500] loss: 0.0144\n",
            " [3, 11000] loss: 0.0144\n",
            " validation loss: 0.0157\n",
            " validation accuracy: 0.7760\n",
            " [4,     1] loss: 0.0000\n",
            " [4,   500] loss: 0.0129\n",
            " [4,  1000] loss: 0.0132\n",
            " [4,  1500] loss: 0.0129\n",
            " [4,  2000] loss: 0.0130\n",
            " [4,  2500] loss: 0.0132\n",
            " [4,  3000] loss: 0.0131\n",
            " [4,  3500] loss: 0.0131\n",
            " [4,  4000] loss: 0.0132\n",
            " [4,  4500] loss: 0.0133\n",
            " [4,  5000] loss: 0.0131\n",
            " [4,  5500] loss: 0.0131\n",
            " [4,  6000] loss: 0.0135\n",
            " [4,  6500] loss: 0.0133\n",
            " [4,  7000] loss: 0.0131\n",
            " [4,  7500] loss: 0.0131\n",
            " [4,  8000] loss: 0.0132\n",
            " [4,  8500] loss: 0.0134\n",
            " [4,  9000] loss: 0.0132\n",
            " [4,  9500] loss: 0.0134\n",
            " [4, 10000] loss: 0.0134\n",
            " [4, 10500] loss: 0.0135\n",
            " [4, 11000] loss: 0.0134\n",
            " validation loss: 0.0157\n",
            " validation accuracy: 0.7695\n",
            " [5,     1] loss: 0.0000\n",
            " [5,   500] loss: 0.0119\n",
            " [5,  1000] loss: 0.0121\n",
            " [5,  1500] loss: 0.0121\n",
            " [5,  2000] loss: 0.0119\n",
            " [5,  2500] loss: 0.0120\n",
            " [5,  3000] loss: 0.0120\n",
            " [5,  3500] loss: 0.0122\n",
            " [5,  4000] loss: 0.0121\n",
            " [5,  4500] loss: 0.0120\n",
            " [5,  5000] loss: 0.0119\n",
            " [5,  5500] loss: 0.0122\n",
            " [5,  6000] loss: 0.0123\n",
            " [5,  6500] loss: 0.0124\n",
            " [5,  7000] loss: 0.0124\n",
            " [5,  7500] loss: 0.0122\n",
            " [5,  8000] loss: 0.0122\n",
            " [5,  8500] loss: 0.0125\n",
            " [5,  9000] loss: 0.0122\n",
            " [5,  9500] loss: 0.0124\n",
            " [5, 10000] loss: 0.0126\n",
            " [5, 10500] loss: 0.0124\n",
            " [5, 11000] loss: 0.0126\n",
            " validation loss: 0.0156\n",
            " validation accuracy: 0.7799\n",
            " [6,     1] loss: 0.0000\n",
            " [6,   500] loss: 0.0109\n",
            " [6,  1000] loss: 0.0110\n",
            " [6,  1500] loss: 0.0110\n",
            " [6,  2000] loss: 0.0111\n",
            " [6,  2500] loss: 0.0112\n",
            " [6,  3000] loss: 0.0110\n",
            " [6,  3500] loss: 0.0110\n",
            " [6,  4000] loss: 0.0111\n",
            " [6,  4500] loss: 0.0115\n",
            " [6,  5000] loss: 0.0112\n",
            " [6,  5500] loss: 0.0112\n",
            " [6,  6000] loss: 0.0112\n",
            " [6,  6500] loss: 0.0116\n",
            " [6,  7000] loss: 0.0115\n",
            " [6,  7500] loss: 0.0115\n",
            " [6,  8000] loss: 0.0114\n",
            " [6,  8500] loss: 0.0113\n",
            " [6,  9000] loss: 0.0118\n",
            " [6,  9500] loss: 0.0116\n",
            " [6, 10000] loss: 0.0117\n",
            " [6, 10500] loss: 0.0116\n",
            " [6, 11000] loss: 0.0116\n",
            " validation loss: 0.0167\n",
            " validation accuracy: 0.7804\n",
            " [7,     1] loss: 0.0000\n",
            " [7,   500] loss: 0.0097\n",
            " [7,  1000] loss: 0.0100\n",
            " [7,  1500] loss: 0.0102\n",
            " [7,  2000] loss: 0.0102\n",
            " [7,  2500] loss: 0.0103\n",
            " [7,  3000] loss: 0.0102\n",
            " [7,  3500] loss: 0.0103\n",
            " [7,  4000] loss: 0.0105\n",
            " [7,  4500] loss: 0.0105\n",
            " [7,  5000] loss: 0.0103\n",
            " [7,  5500] loss: 0.0107\n",
            " [7,  6000] loss: 0.0105\n",
            " [7,  6500] loss: 0.0109\n",
            " [7,  7000] loss: 0.0107\n",
            " [7,  7500] loss: 0.0106\n",
            " [7,  8000] loss: 0.0107\n",
            " [7,  8500] loss: 0.0107\n",
            " [7,  9000] loss: 0.0108\n",
            " [7,  9500] loss: 0.0109\n",
            " [7, 10000] loss: 0.0110\n",
            " [7, 10500] loss: 0.0109\n",
            " [7, 11000] loss: 0.0109\n",
            " validation loss: 0.0165\n",
            " validation accuracy: 0.7784\n",
            " [8,     1] loss: 0.0000\n",
            " [8,   500] loss: 0.0089\n",
            " [8,  1000] loss: 0.0091\n",
            " [8,  1500] loss: 0.0093\n",
            " [8,  2000] loss: 0.0095\n",
            " [8,  2500] loss: 0.0097\n",
            " [8,  3000] loss: 0.0093\n",
            " [8,  3500] loss: 0.0098\n",
            " [8,  4000] loss: 0.0097\n",
            " [8,  4500] loss: 0.0099\n",
            " [8,  5000] loss: 0.0100\n",
            " [8,  5500] loss: 0.0098\n",
            " [8,  6000] loss: 0.0099\n",
            " [8,  6500] loss: 0.0100\n",
            " [8,  7000] loss: 0.0101\n",
            " [8,  7500] loss: 0.0101\n",
            " [8,  8000] loss: 0.0100\n",
            " [8,  8500] loss: 0.0102\n",
            " [8,  9000] loss: 0.0102\n",
            " [8,  9500] loss: 0.0100\n",
            " [8, 10000] loss: 0.0104\n",
            " [8, 10500] loss: 0.0104\n",
            " [8, 11000] loss: 0.0105\n",
            " validation loss: 0.0178\n",
            " validation accuracy: 0.7740\n",
            " [9,     1] loss: 0.0000\n",
            " [9,   500] loss: 0.0086\n",
            " [9,  1000] loss: 0.0087\n",
            " [9,  1500] loss: 0.0088\n",
            " [9,  2000] loss: 0.0087\n",
            " [9,  2500] loss: 0.0089\n",
            " [9,  3000] loss: 0.0090\n",
            " [9,  3500] loss: 0.0091\n",
            " [9,  4000] loss: 0.0092\n",
            " [9,  4500] loss: 0.0093\n",
            " [9,  5000] loss: 0.0091\n",
            " [9,  5500] loss: 0.0093\n",
            " [9,  6000] loss: 0.0090\n",
            " [9,  6500] loss: 0.0094\n",
            " [9,  7000] loss: 0.0095\n",
            " [9,  7500] loss: 0.0094\n",
            " [9,  8000] loss: 0.0095\n",
            " [9,  8500] loss: 0.0096\n",
            " [9,  9000] loss: 0.0096\n",
            " [9,  9500] loss: 0.0096\n",
            " [9, 10000] loss: 0.0095\n",
            " [9, 10500] loss: 0.0098\n",
            " [9, 11000] loss: 0.0097\n",
            " validation loss: 0.0186\n",
            " validation accuracy: 0.7750\n",
            " [10,     1] loss: 0.0000\n",
            " [10,   500] loss: 0.0081\n",
            " [10,  1000] loss: 0.0082\n",
            " [10,  1500] loss: 0.0082\n",
            " [10,  2000] loss: 0.0083\n",
            " [10,  2500] loss: 0.0082\n",
            " [10,  3000] loss: 0.0085\n",
            " [10,  3500] loss: 0.0086\n",
            " [10,  4000] loss: 0.0085\n",
            " [10,  4500] loss: 0.0086\n",
            " [10,  5000] loss: 0.0091\n",
            " [10,  5500] loss: 0.0085\n",
            " [10,  6000] loss: 0.0089\n",
            " [10,  6500] loss: 0.0090\n",
            " [10,  7000] loss: 0.0091\n",
            " [10,  7500] loss: 0.0089\n",
            " [10,  8000] loss: 0.0092\n",
            " [10,  8500] loss: 0.0088\n",
            " [10,  9000] loss: 0.0091\n",
            " [10,  9500] loss: 0.0091\n",
            " [10, 10000] loss: 0.0093\n",
            " [10, 10500] loss: 0.0094\n",
            " [10, 11000] loss: 0.0091\n",
            " validation loss: 0.0191\n",
            " validation accuracy: 0.7809\n",
            " [11,     1] loss: 0.0000\n",
            " [11,   500] loss: 0.0077\n",
            " [11,  1000] loss: 0.0076\n",
            " [11,  1500] loss: 0.0077\n",
            " [11,  2000] loss: 0.0078\n",
            " [11,  2500] loss: 0.0080\n",
            " [11,  3000] loss: 0.0080\n",
            " [11,  3500] loss: 0.0080\n",
            " [11,  4000] loss: 0.0081\n",
            " [11,  4500] loss: 0.0083\n",
            " [11,  5000] loss: 0.0086\n",
            " [11,  5500] loss: 0.0084\n",
            " [11,  6000] loss: 0.0084\n",
            " [11,  6500] loss: 0.0085\n",
            " [11,  7000] loss: 0.0084\n",
            " [11,  7500] loss: 0.0088\n",
            " [11,  8000] loss: 0.0087\n",
            " [11,  8500] loss: 0.0085\n",
            " [11,  9000] loss: 0.0088\n",
            " [11,  9500] loss: 0.0085\n",
            " [11, 10000] loss: 0.0089\n",
            " [11, 10500] loss: 0.0089\n",
            " [11, 11000] loss: 0.0089\n",
            " validation loss: 0.0193\n",
            " validation accuracy: 0.7720\n",
            " [12,     1] loss: 0.0000\n",
            " [12,   500] loss: 0.0072\n",
            " [12,  1000] loss: 0.0074\n",
            " [12,  1500] loss: 0.0075\n",
            " [12,  2000] loss: 0.0075\n",
            " [12,  2500] loss: 0.0075\n",
            " [12,  3000] loss: 0.0077\n",
            " [12,  3500] loss: 0.0078\n",
            " [12,  4000] loss: 0.0079\n",
            " [12,  4500] loss: 0.0078\n",
            " [12,  5000] loss: 0.0078\n",
            " [12,  5500] loss: 0.0081\n",
            " [12,  6000] loss: 0.0083\n",
            " [12,  6500] loss: 0.0082\n",
            " [12,  7000] loss: 0.0080\n",
            " [12,  7500] loss: 0.0081\n",
            " [12,  8000] loss: 0.0083\n",
            " [12,  8500] loss: 0.0085\n",
            " [12,  9000] loss: 0.0084\n",
            " [12,  9500] loss: 0.0082\n",
            " [12, 10000] loss: 0.0084\n",
            " [12, 10500] loss: 0.0081\n",
            " [12, 11000] loss: 0.0085\n",
            " validation loss: 0.0215\n",
            " validation accuracy: 0.7760\n",
            " [13,     1] loss: 0.0000\n",
            " [13,   500] loss: 0.0067\n",
            " [13,  1000] loss: 0.0069\n",
            " [13,  1500] loss: 0.0071\n",
            " [13,  2000] loss: 0.0071\n",
            " [13,  2500] loss: 0.0072\n",
            " [13,  3000] loss: 0.0073\n",
            " [13,  3500] loss: 0.0073\n",
            " [13,  4000] loss: 0.0076\n",
            " [13,  4500] loss: 0.0075\n",
            " [13,  5000] loss: 0.0075\n",
            " [13,  5500] loss: 0.0078\n",
            " [13,  6000] loss: 0.0078\n",
            " [13,  6500] loss: 0.0079\n",
            " [13,  7000] loss: 0.0077\n",
            " [13,  7500] loss: 0.0079\n",
            " [13,  8000] loss: 0.0077\n",
            " [13,  8500] loss: 0.0079\n",
            " [13,  9000] loss: 0.0079\n",
            " [13,  9500] loss: 0.0080\n",
            " [13, 10000] loss: 0.0082\n",
            " [13, 10500] loss: 0.0081\n",
            " [13, 11000] loss: 0.0082\n",
            " validation loss: 0.0207\n",
            " validation accuracy: 0.7676\n",
            " [14,     1] loss: 0.0000\n",
            " [14,   500] loss: 0.0064\n",
            " [14,  1000] loss: 0.0065\n",
            " [14,  1500] loss: 0.0068\n",
            " [14,  2000] loss: 0.0067\n",
            " [14,  2500] loss: 0.0070\n",
            " [14,  3000] loss: 0.0068\n",
            " [14,  3500] loss: 0.0069\n",
            " [14,  4000] loss: 0.0070\n",
            " [14,  4500] loss: 0.0073\n",
            " [14,  5000] loss: 0.0072\n",
            " [14,  5500] loss: 0.0074\n",
            " [14,  6000] loss: 0.0075\n",
            " [14,  6500] loss: 0.0074\n",
            " [14,  7000] loss: 0.0075\n",
            " [14,  7500] loss: 0.0075\n",
            " [14,  8000] loss: 0.0075\n",
            " [14,  8500] loss: 0.0076\n",
            " [14,  9000] loss: 0.0077\n",
            " [14,  9500] loss: 0.0078\n",
            " [14, 10000] loss: 0.0079\n",
            " [14, 10500] loss: 0.0078\n",
            " [14, 11000] loss: 0.0078\n",
            " validation loss: 0.0213\n",
            " validation accuracy: 0.7710\n",
            " [15,     1] loss: 0.0000\n",
            " [15,   500] loss: 0.0061\n",
            " [15,  1000] loss: 0.0065\n",
            " [15,  1500] loss: 0.0063\n",
            " [15,  2000] loss: 0.0064\n",
            " [15,  2500] loss: 0.0066\n",
            " [15,  3000] loss: 0.0065\n",
            " [15,  3500] loss: 0.0068\n",
            " [15,  4000] loss: 0.0070\n",
            " [15,  4500] loss: 0.0072\n",
            " [15,  5000] loss: 0.0069\n",
            " [15,  5500] loss: 0.0073\n",
            " [15,  6000] loss: 0.0074\n",
            " [15,  6500] loss: 0.0072\n",
            " [15,  7000] loss: 0.0073\n",
            " [15,  7500] loss: 0.0070\n",
            " [15,  8000] loss: 0.0073\n",
            " [15,  8500] loss: 0.0073\n",
            " [15,  9000] loss: 0.0073\n",
            " [15,  9500] loss: 0.0075\n",
            " [15, 10000] loss: 0.0074\n",
            " [15, 10500] loss: 0.0074\n",
            " [15, 11000] loss: 0.0075\n",
            " validation loss: 0.0221\n",
            " validation accuracy: 0.7725\n",
            " [16,     1] loss: 0.0000\n",
            " [16,   500] loss: 0.0060\n",
            " [16,  1000] loss: 0.0060\n",
            " [16,  1500] loss: 0.0062\n",
            " [16,  2000] loss: 0.0061\n",
            " [16,  2500] loss: 0.0063\n",
            " [16,  3000] loss: 0.0064\n",
            " [16,  3500] loss: 0.0066\n",
            " [16,  4000] loss: 0.0068\n",
            " [16,  4500] loss: 0.0067\n",
            " [16,  5000] loss: 0.0066\n",
            " [16,  5500] loss: 0.0068\n",
            " [16,  6000] loss: 0.0067\n",
            " [16,  6500] loss: 0.0071\n",
            " [16,  7000] loss: 0.0071\n",
            " [16,  7500] loss: 0.0068\n",
            " [16,  8000] loss: 0.0072\n",
            " [16,  8500] loss: 0.0070\n",
            " [16,  9000] loss: 0.0071\n",
            " [16,  9500] loss: 0.0074\n",
            " [16, 10000] loss: 0.0072\n",
            " [16, 10500] loss: 0.0073\n",
            " [16, 11000] loss: 0.0073\n",
            " validation loss: 0.0232\n",
            " validation accuracy: 0.7770\n",
            " [17,     1] loss: 0.0000\n",
            " [17,   500] loss: 0.0058\n",
            " [17,  1000] loss: 0.0058\n",
            " [17,  1500] loss: 0.0060\n",
            " [17,  2000] loss: 0.0061\n",
            " [17,  2500] loss: 0.0061\n",
            " [17,  3000] loss: 0.0061\n",
            " [17,  3500] loss: 0.0063\n",
            " [17,  4000] loss: 0.0065\n",
            " [17,  4500] loss: 0.0065\n",
            " [17,  5000] loss: 0.0067\n",
            " [17,  5500] loss: 0.0066\n",
            " [17,  6000] loss: 0.0066\n",
            " [17,  6500] loss: 0.0065\n",
            " [17,  7000] loss: 0.0067\n",
            " [17,  7500] loss: 0.0069\n",
            " [17,  8000] loss: 0.0070\n",
            " [17,  8500] loss: 0.0071\n",
            " [17,  9000] loss: 0.0069\n",
            " [17,  9500] loss: 0.0071\n",
            " [17, 10000] loss: 0.0070\n",
            " [17, 10500] loss: 0.0068\n",
            " [17, 11000] loss: 0.0071\n",
            " validation loss: 0.0239\n",
            " validation accuracy: 0.7789\n",
            " [18,     1] loss: 0.0000\n",
            " [18,   500] loss: 0.0054\n",
            " [18,  1000] loss: 0.0055\n",
            " [18,  1500] loss: 0.0060\n",
            " [18,  2000] loss: 0.0059\n",
            " [18,  2500] loss: 0.0061\n",
            " [18,  3000] loss: 0.0063\n",
            " [18,  3500] loss: 0.0060\n",
            " [18,  4000] loss: 0.0062\n",
            " [18,  4500] loss: 0.0063\n",
            " [18,  5000] loss: 0.0064\n",
            " [18,  5500] loss: 0.0063\n",
            " [18,  6000] loss: 0.0068\n",
            " [18,  6500] loss: 0.0066\n",
            " [18,  7000] loss: 0.0069\n",
            " [18,  7500] loss: 0.0066\n",
            " [18,  8000] loss: 0.0065\n",
            " [18,  8500] loss: 0.0067\n",
            " [18,  9000] loss: 0.0068\n",
            " [18,  9500] loss: 0.0066\n",
            " [18, 10000] loss: 0.0067\n",
            " [18, 10500] loss: 0.0069\n",
            " [18, 11000] loss: 0.0068\n",
            " validation loss: 0.0240\n",
            " validation accuracy: 0.7695\n",
            " [19,     1] loss: 0.0000\n",
            " [19,   500] loss: 0.0054\n",
            " [19,  1000] loss: 0.0056\n",
            " [19,  1500] loss: 0.0057\n",
            " [19,  2000] loss: 0.0057\n",
            " [19,  2500] loss: 0.0058\n",
            " [19,  3000] loss: 0.0061\n",
            " [19,  3500] loss: 0.0062\n",
            " [19,  4000] loss: 0.0061\n",
            " [19,  4500] loss: 0.0060\n",
            " [19,  5000] loss: 0.0059\n",
            " [19,  5500] loss: 0.0059\n",
            " [19,  6000] loss: 0.0061\n",
            " [19,  6500] loss: 0.0062\n",
            " [19,  7000] loss: 0.0065\n",
            " [19,  7500] loss: 0.0065\n",
            " [19,  8000] loss: 0.0064\n",
            " [19,  8500] loss: 0.0065\n",
            " [19,  9000] loss: 0.0067\n",
            " [19,  9500] loss: 0.0065\n",
            " [19, 10000] loss: 0.0070\n",
            " [19, 10500] loss: 0.0066\n",
            " [19, 11000] loss: 0.0068\n",
            " validation loss: 0.0251\n",
            " validation accuracy: 0.7755\n",
            " [20,     1] loss: 0.0000\n",
            " [20,   500] loss: 0.0054\n",
            " [20,  1000] loss: 0.0054\n",
            " [20,  1500] loss: 0.0057\n",
            " [20,  2000] loss: 0.0058\n",
            " [20,  2500] loss: 0.0057\n",
            " [20,  3000] loss: 0.0059\n",
            " [20,  3500] loss: 0.0057\n",
            " [20,  4000] loss: 0.0059\n",
            " [20,  4500] loss: 0.0059\n",
            " [20,  5000] loss: 0.0058\n",
            " [20,  5500] loss: 0.0060\n",
            " [20,  6000] loss: 0.0062\n",
            " [20,  6500] loss: 0.0065\n",
            " [20,  7000] loss: 0.0062\n",
            " [20,  7500] loss: 0.0062\n",
            " [20,  8000] loss: 0.0063\n",
            " [20,  8500] loss: 0.0062\n",
            " [20,  9000] loss: 0.0063\n",
            " [20,  9500] loss: 0.0063\n",
            " [20, 10000] loss: 0.0066\n",
            " [20, 10500] loss: 0.0064\n",
            " [20, 11000] loss: 0.0065\n",
            " validation loss: 0.0249\n",
            " validation accuracy: 0.7671\n",
            " [21,     1] loss: 0.0000\n",
            " [21,   500] loss: 0.0052\n",
            " [21,  1000] loss: 0.0049\n",
            " [21,  1500] loss: 0.0052\n",
            " [21,  2000] loss: 0.0055\n",
            " [21,  2500] loss: 0.0058\n",
            " [21,  3000] loss: 0.0055\n",
            " [21,  3500] loss: 0.0056\n",
            " [21,  4000] loss: 0.0058\n",
            " [21,  4500] loss: 0.0057\n",
            " [21,  5000] loss: 0.0057\n",
            " [21,  5500] loss: 0.0060\n",
            " [21,  6000] loss: 0.0060\n",
            " [21,  6500] loss: 0.0059\n",
            " [21,  7000] loss: 0.0059\n",
            " [21,  7500] loss: 0.0061\n",
            " [21,  8000] loss: 0.0062\n",
            " [21,  8500] loss: 0.0061\n",
            " [21,  9000] loss: 0.0062\n",
            " [21,  9500] loss: 0.0064\n",
            " [21, 10000] loss: 0.0062\n",
            " [21, 10500] loss: 0.0065\n",
            " [21, 11000] loss: 0.0062\n",
            " validation loss: 0.0246\n",
            " validation accuracy: 0.7819\n",
            " [22,     1] loss: 0.0000\n",
            " [22,   500] loss: 0.0050\n",
            " [22,  1000] loss: 0.0051\n",
            " [22,  1500] loss: 0.0051\n",
            " [22,  2000] loss: 0.0054\n",
            " [22,  2500] loss: 0.0053\n",
            " [22,  3000] loss: 0.0054\n",
            " [22,  3500] loss: 0.0056\n",
            " [22,  4000] loss: 0.0053\n",
            " [22,  4500] loss: 0.0058\n",
            " [22,  5000] loss: 0.0056\n",
            " [22,  5500] loss: 0.0057\n",
            " [22,  6000] loss: 0.0059\n",
            " [22,  6500] loss: 0.0061\n",
            " [22,  7000] loss: 0.0060\n",
            " [22,  7500] loss: 0.0062\n",
            " [22,  8000] loss: 0.0058\n",
            " [22,  8500] loss: 0.0059\n",
            " [22,  9000] loss: 0.0061\n",
            " [22,  9500] loss: 0.0062\n",
            " [22, 10000] loss: 0.0062\n",
            " [22, 10500] loss: 0.0062\n",
            " [22, 11000] loss: 0.0064\n",
            " validation loss: 0.0250\n",
            " validation accuracy: 0.7740\n",
            " [23,     1] loss: 0.0000\n",
            " [23,   500] loss: 0.0047\n",
            " [23,  1000] loss: 0.0051\n",
            " [23,  1500] loss: 0.0053\n",
            " [23,  2000] loss: 0.0054\n",
            " [23,  2500] loss: 0.0053\n",
            " [23,  3000] loss: 0.0053\n",
            " [23,  3500] loss: 0.0053\n",
            " [23,  4000] loss: 0.0055\n",
            " [23,  4500] loss: 0.0056\n",
            " [23,  5000] loss: 0.0057\n",
            " [23,  5500] loss: 0.0057\n",
            " [23,  6000] loss: 0.0055\n",
            " [23,  6500] loss: 0.0055\n",
            " [23,  7000] loss: 0.0058\n",
            " [23,  7500] loss: 0.0057\n",
            " [23,  8000] loss: 0.0057\n",
            " [23,  8500] loss: 0.0059\n",
            " [23,  9000] loss: 0.0061\n",
            " [23,  9500] loss: 0.0060\n",
            " [23, 10000] loss: 0.0061\n",
            " [23, 10500] loss: 0.0061\n",
            " [23, 11000] loss: 0.0060\n",
            " validation loss: 0.0254\n",
            " validation accuracy: 0.7824\n",
            " [24,     1] loss: 0.0000\n",
            " [24,   500] loss: 0.0048\n",
            " [24,  1000] loss: 0.0048\n",
            " [24,  1500] loss: 0.0050\n",
            " [24,  2000] loss: 0.0049\n",
            " [24,  2500] loss: 0.0051\n",
            " [24,  3000] loss: 0.0052\n",
            " [24,  3500] loss: 0.0052\n",
            " [24,  4000] loss: 0.0053\n",
            " [24,  4500] loss: 0.0056\n",
            " [24,  5000] loss: 0.0053\n",
            " [24,  5500] loss: 0.0056\n",
            " [24,  6000] loss: 0.0055\n",
            " [24,  6500] loss: 0.0054\n",
            " [24,  7000] loss: 0.0055\n",
            " [24,  7500] loss: 0.0057\n",
            " [24,  8000] loss: 0.0056\n",
            " [24,  8500] loss: 0.0059\n",
            " [24,  9000] loss: 0.0058\n",
            " [24,  9500] loss: 0.0060\n",
            " [24, 10000] loss: 0.0059\n",
            " [24, 10500] loss: 0.0059\n",
            " [24, 11000] loss: 0.0060\n",
            " validation loss: 0.0255\n",
            " validation accuracy: 0.7819\n",
            " [25,     1] loss: 0.0000\n",
            " [25,   500] loss: 0.0047\n",
            " [25,  1000] loss: 0.0049\n",
            " [25,  1500] loss: 0.0048\n",
            " [25,  2000] loss: 0.0049\n",
            " [25,  2500] loss: 0.0049\n",
            " [25,  3000] loss: 0.0053\n",
            " [25,  3500] loss: 0.0053\n",
            " [25,  4000] loss: 0.0053\n",
            " [25,  4500] loss: 0.0055\n",
            " [25,  5000] loss: 0.0052\n",
            " [25,  5500] loss: 0.0055\n",
            " [25,  6000] loss: 0.0055\n",
            " [25,  6500] loss: 0.0052\n",
            " [25,  7000] loss: 0.0055\n",
            " [25,  7500] loss: 0.0055\n",
            " [25,  8000] loss: 0.0056\n",
            " [25,  8500] loss: 0.0055\n",
            " [25,  9000] loss: 0.0057\n",
            " [25,  9500] loss: 0.0058\n",
            " [25, 10000] loss: 0.0057\n",
            " [25, 10500] loss: 0.0059\n",
            " [25, 11000] loss: 0.0058\n",
            " validation loss: 0.0280\n",
            " validation accuracy: 0.7774\n",
            " [26,     1] loss: 0.0000\n",
            " [26,   500] loss: 0.0048\n",
            " [26,  1000] loss: 0.0048\n",
            " [26,  1500] loss: 0.0046\n",
            " [26,  2000] loss: 0.0050\n",
            " [26,  2500] loss: 0.0050\n",
            " [26,  3000] loss: 0.0051\n",
            " [26,  3500] loss: 0.0051\n",
            " [26,  4000] loss: 0.0052\n",
            " [26,  4500] loss: 0.0053\n",
            " [26,  5000] loss: 0.0052\n",
            " [26,  5500] loss: 0.0051\n",
            " [26,  6000] loss: 0.0054\n",
            " [26,  6500] loss: 0.0052\n",
            " [26,  7000] loss: 0.0055\n",
            " [26,  7500] loss: 0.0056\n",
            " [26,  8000] loss: 0.0053\n",
            " [26,  8500] loss: 0.0055\n",
            " [26,  9000] loss: 0.0055\n",
            " [26,  9500] loss: 0.0055\n",
            " [26, 10000] loss: 0.0057\n",
            " [26, 10500] loss: 0.0055\n",
            " [26, 11000] loss: 0.0057\n",
            " validation loss: 0.0285\n",
            " validation accuracy: 0.7829\n",
            " [27,     1] loss: 0.0000\n",
            " [27,   500] loss: 0.0046\n",
            " [27,  1000] loss: 0.0045\n",
            " [27,  1500] loss: 0.0047\n",
            " [27,  2000] loss: 0.0048\n",
            " [27,  2500] loss: 0.0049\n",
            " [27,  3000] loss: 0.0049\n",
            " [27,  3500] loss: 0.0049\n",
            " [27,  4000] loss: 0.0051\n",
            " [27,  4500] loss: 0.0051\n",
            " [27,  5000] loss: 0.0054\n",
            " [27,  5500] loss: 0.0052\n",
            " [27,  6000] loss: 0.0053\n",
            " [27,  6500] loss: 0.0051\n",
            " [27,  7000] loss: 0.0053\n",
            " [27,  7500] loss: 0.0056\n",
            " [27,  8000] loss: 0.0054\n",
            " [27,  8500] loss: 0.0055\n",
            " [27,  9000] loss: 0.0054\n",
            " [27,  9500] loss: 0.0055\n",
            " [27, 10000] loss: 0.0056\n",
            " [27, 10500] loss: 0.0056\n",
            " [27, 11000] loss: 0.0058\n",
            " validation loss: 0.0267\n",
            " validation accuracy: 0.7760\n",
            " [28,     1] loss: 0.0000\n",
            " [28,   500] loss: 0.0043\n",
            " [28,  1000] loss: 0.0044\n",
            " [28,  1500] loss: 0.0047\n",
            " [28,  2000] loss: 0.0047\n",
            " [28,  2500] loss: 0.0048\n",
            " [28,  3000] loss: 0.0050\n",
            " [28,  3500] loss: 0.0049\n",
            " [28,  4000] loss: 0.0049\n",
            " [28,  4500] loss: 0.0050\n",
            " [28,  5000] loss: 0.0052\n",
            " [28,  5500] loss: 0.0049\n",
            " [28,  6000] loss: 0.0050\n",
            " [28,  6500] loss: 0.0051\n",
            " [28,  7000] loss: 0.0052\n",
            " [28,  7500] loss: 0.0051\n",
            " [28,  8000] loss: 0.0052\n",
            " [28,  8500] loss: 0.0054\n",
            " [28,  9000] loss: 0.0056\n",
            " [28,  9500] loss: 0.0055\n",
            " [28, 10000] loss: 0.0054\n",
            " [28, 10500] loss: 0.0053\n",
            " [28, 11000] loss: 0.0055\n",
            " validation loss: 0.0283\n",
            " validation accuracy: 0.7760\n",
            " [29,     1] loss: 0.0000\n",
            " [29,   500] loss: 0.0043\n",
            " [29,  1000] loss: 0.0044\n",
            " [29,  1500] loss: 0.0047\n",
            " [29,  2000] loss: 0.0047\n",
            " [29,  2500] loss: 0.0047\n",
            " [29,  3000] loss: 0.0047\n",
            " [29,  3500] loss: 0.0047\n",
            " [29,  4000] loss: 0.0049\n",
            " [29,  4500] loss: 0.0047\n",
            " [29,  5000] loss: 0.0051\n",
            " [29,  5500] loss: 0.0050\n",
            " [29,  6000] loss: 0.0052\n",
            " [29,  6500] loss: 0.0051\n",
            " [29,  7000] loss: 0.0051\n",
            " [29,  7500] loss: 0.0052\n",
            " [29,  8000] loss: 0.0051\n",
            " [29,  8500] loss: 0.0051\n",
            " [29,  9000] loss: 0.0053\n",
            " [29,  9500] loss: 0.0052\n",
            " [29, 10000] loss: 0.0054\n",
            " [29, 10500] loss: 0.0056\n",
            " [29, 11000] loss: 0.0054\n",
            " validation loss: 0.0297\n",
            " validation accuracy: 0.7784\n",
            " [30,     1] loss: 0.0000\n",
            " [30,   500] loss: 0.0042\n",
            " [30,  1000] loss: 0.0043\n",
            " [30,  1500] loss: 0.0044\n",
            " [30,  2000] loss: 0.0047\n",
            " [30,  2500] loss: 0.0047\n",
            " [30,  3000] loss: 0.0047\n",
            " [30,  3500] loss: 0.0048\n",
            " [30,  4000] loss: 0.0047\n",
            " [30,  4500] loss: 0.0046\n",
            " [30,  5000] loss: 0.0052\n",
            " [30,  5500] loss: 0.0050\n",
            " [30,  6000] loss: 0.0050\n",
            " [30,  6500] loss: 0.0051\n",
            " [30,  7000] loss: 0.0050\n",
            " [30,  7500] loss: 0.0050\n",
            " [30,  8000] loss: 0.0053\n",
            " [30,  8500] loss: 0.0054\n",
            " [30,  9000] loss: 0.0053\n",
            " [30,  9500] loss: 0.0050\n",
            " [30, 10000] loss: 0.0055\n",
            " [30, 10500] loss: 0.0052\n",
            " [30, 11000] loss: 0.0052\n",
            " validation loss: 0.0297\n",
            " validation accuracy: 0.7824\n",
            "     6710.487154846998\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "temps1 = ti.perf_counter()\n",
        "total = len(train_set)\n",
        "dis_loss = 500\n",
        "for epoch in range(n_epochs):\n",
        "    running_loss = 0\n",
        "    model_simple.train()\n",
        "    for i, data in enumerate(trainloader):\n",
        "        x_dic, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_simple(x_dic).T\n",
        "        loss = criterion(outputs, labels.cuda())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 20 == 0:\n",
        "            print('\\r', '{:.2%}'.format(i / (total // bs)), end = '', sep = '')\n",
        "        if i % dis_loss == dis_loss - 1 or i == 0:    # print every dis_loss mini-batches\n",
        "            print('\\r', f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / dis_loss / bs:.4f}')\n",
        "            running_loss = 0.0\n",
        "        # if i == 0:\n",
        "        #     break\n",
        "\n",
        "    running_loss = 0\n",
        "    running_accuracy = 0\n",
        "    model_simple.eval()\n",
        "    for i, data in enumerate(valid_loader):\n",
        "        x_dic, labels = data\n",
        "        outputs = model_simple(x_dic).T\n",
        "        loss = criterion(outputs, labels.cuda())\n",
        "        running_loss += loss.item()\n",
        "        sigmoid = nn.Sigmoid()(outputs)\n",
        "        predictions = torch.round(sigmoid)\n",
        "        running_accuracy += (predictions == labels.cuda()).sum()\n",
        "    print('\\r', f\"validation loss: {running_loss / df_valid.shape[0]:.4f}\")\n",
        "    print('\\r', f\"validation accuracy: {running_accuracy / df_valid.shape[0]:.4f}\")\n",
        "\n",
        "print(\"    \", ti.perf_counter() - temps1)\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "zsUb2wCANufp"
      },
      "outputs": [],
      "source": [
        "#Possibility to verify on the test part\n",
        "test_set = Quora_dataset2(df_test, vocab)\n",
        "\n",
        "bs = 32\n",
        "params = {'batch_size' : bs,\n",
        "          'shuffle' : True,\n",
        "          'num_workers' : 0,\n",
        "          'collate_fn' : custom_collate}\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set, **params)\n",
        "\n",
        "running_loss = 0\n",
        "running_accuracy = 0\n",
        "model_simple.eval()\n",
        "for i, data in enumerate(test_loader):\n",
        "    x_dic, labels = data\n",
        "    outputs = model_simple(x_dic).T\n",
        "    loss = criterion(outputs, labels.cuda())\n",
        "    running_loss += loss.item()\n",
        "    sigmoid = nn.Sigmoid()(outputs)\n",
        "    predictions = torch.round(sigmoid)\n",
        "    running_accuracy += (predictions == labels.cuda()).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBTee1-DnC01",
        "outputId": "f241c293-31ea-4576-c286-d5239563df75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r testing accuracy: 0.7822\n"
          ]
        }
      ],
      "source": [
        "print('\\r', f\"testing accuracy: {running_accuracy / df_test.shape[0]:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Quora4_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
